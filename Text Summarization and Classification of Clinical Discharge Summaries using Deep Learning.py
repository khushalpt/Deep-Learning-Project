# -*- coding: utf-8 -*-
"""Research_CNN (4).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/146x9PwWd6SpYqR8o_pD6JYIEhOOMlQRV
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import pickle
from collections import defaultdict
import re
from bs4 import BeautifulSoup
import sys
import os
os.environ['KERAS_BACKEND']='theano' # Why theano why not
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical
from keras.layers import Embedding
from keras.layers import Dense, Input, Flatten
from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, BatchNormalization
from keras.models import Model
from keras.callbacks import ModelCheckpoint
from collections import Counter
from sklearn.datasets import make_classification
from sklearn import datasets, linear_model
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
from keras.utils.vis_utils import plot_model
plt.switch_backend('agg')

length = 1000
words = 20000
dim = 100

df = pd.read_excel('/content/drive/My Drive/dataset/mtsamples_v2_exel.xlsx')
df = df.dropna()
df = df.reset_index(drop=True)
print('Input Shape ',df.shape)
# print(df.columns)
print('No. of Classes',len(set(df['medical_specialty'])))
print(sorted(set(df['medical_specialty'])))
# Set it to None to display all columns in the dataframe
# pd.set_option('display.max_columns', None)
# print(empDfObj)

num=sorted(set(df['medical_specialty']))
nid = dict((note, number) for number, note in enumerate(num))
def fnid(i):
    return nid[i]

df['medical_specialty']=df['medical_specialty'].apply(fnid)
print(sorted(set(df['medical_specialty'])))
# print(df['medical_specialty'].shape)

texts = []
labels = []
def clean_str(string):
    
    string = re.sub(r"\\", "", string)
    string = re.sub(r"\"", "", string)
    string = re.sub(r"\'", "", string)
    
    return string.strip().lower()

for index in range(df.keywords.shape[0]):
    text = BeautifulSoup(df.keywords[index])
    texts.append(clean_str(str(text.get_text().encode())))

for index in df['medical_specialty']:
    labels.append(index)

token = Tokenizer(num_words=length)
token.fit_on_texts(texts)
seq = token.texts_to_sequences(texts)
word_index = token.word_index
print('No. tokens: ',len(word_index))

data = pad_sequences(seq, maxlen=length)
print("Input: ", data.shape)
labels = to_categorical(np.asarray(labels))
indxs = np.arange(data.shape[0])
np.random.shuffle(indxs)
data = data[indxs]
labels = labels[indxs]
X_train, x_test, Y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=65)
x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=69)
print("Training samples: ", x_train.shape)
print("Training labels: ", y_train.shape)
print("Testing samples: ", x_test.shape)
print("Testing labels: ", y_test.shape)
print("Validation samples: ", x_val.shape)
print("Validation labels: ", y_val.shape)

embeddings_index = {}
f = open("/content/drive/My Drive/dataset/glove.6B.100d.txt",encoding='utf8')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))

embedding_matrix = np.random.random((len(word_index) + 1, dim))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector

embed_input = Embedding(len(word_index) + 1,
                            dim,weights=[embedding_matrix],
                            input_length=length,trainable=True)

from keras import backend as K

def recall_m(orignal, predicted):
    true_positives = K.sum(K.round(K.clip(orignal * predicted, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(orignal, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

inp = Input(shape=(length,), dtype='int32')

input = embed_input(inp)

x = Conv1D(32, 5, activation='relu')(input)
x = MaxPooling1D(5)(x)
x = Conv1D(64, 5, activation='relu')(x)
x = MaxPooling1D(5)(x)
x = Conv1D(96, 5, activation='relu')(x)
x = MaxPooling1D(5)(x)
x = Dropout(0.2)(x)
x = Flatten()(x)
x = Dropout(0.2)(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.2)(x)
preds = Dense(len(num), activation='softmax')(x)

model = Model(inp, preds)
model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['acc',f1_m,precision_m, recall_m])

model.save('research_CNN.model')
model.summary()

plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)
best_save=ModelCheckpoint('model_cnn.hdf5',monitor='val_acc',verbose=1,save_best_only=True)

history=model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=10, batch_size=2,callbacks=[best_save])

model.load_weights('/content/model_cnn.hdf5')
loss, acc, f1_m, precision_m, recall_m= model.evaluate(x_test,y_test, verbose=0)
print(model.metrics_names)
print("Loss: ", loss)
print("Accuracy: ", acc)
print("F1 score: ", f1_m)
print("Precision: ", precision_m)
print("Recall: ", recall_m)

print(tr_pr.shape)

fig1 = plt.figure()
plt.plot(history.history['loss'],'r')
plt.plot(history.history['val_loss'],'b')
plt.legend(['Training loss', 'Validation Loss'])
plt.xlabel('Epochs ')
plt.ylabel('Loss')
plt.title('Loss Curves :CNN')
fig1.savefig('loss_cnn.png')
plt.show(fig1)

fig2=plt.figure()
plt.plot(history.history['acc'],'r')
plt.plot(history.history['val_acc'],'b')
plt.legend(['Training Accuracy', 'Validation Accuracy'])
plt.xlabel('Epochs ')
plt.ylabel('Accuracy')
plt.title('Accuracy Curves : CNN')
fig2.savefig('accuracy_cnn.png')
plt.show()